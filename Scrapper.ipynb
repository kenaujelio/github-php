{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","\n","url = 'https://www.flipkart.com/search?q=laptop&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&sort=relevance'\n","\n","response = requests.get(url)\n","soup = BeautifulSoup(response.content, 'html.parser')\n","\n","product_listings = soup.find_all('div', class_='_2kHMtA')\n","\n","with open('laptops.csv', mode='w', encoding='utf-8', newline='') as file:\n","    writer = csv.writer(file)\n","    writer.writerow(['Title', 'Price', 'Rating', 'Link'])\n","\n","    # Loop through each product listing and write its details to the CSV file\n","    for product in product_listings:\n","        title = product.find('div', class_='_4rR01T').text.strip()\n","        price = product.find('div', class_='_30jeq3 _1_WHN1').text.strip()[1:].replace(',', '')\n","        rating = product.find('div', class_='_3LWZlK')\n","        rating = rating.text.strip() if rating else ''\n","        link = 'https://www.flipkart.com' + product.find('a', class_='_1fQZEK')['href']\n","        writer.writerow([title, price, rating, link])\n","\n","print('Scraping completed!')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"57ka2utR2576","executionInfo":{"status":"ok","timestamp":1694483512842,"user_tz":-480,"elapsed":2158,"user":{"displayName":"Bea Brolagda","userId":"01319155302746720980"}},"outputId":"5f273a1d-8414-4e58-8ea2-685c41ae958e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scraping completed!\n"]}]},{"cell_type":"markdown","source":["# This scrape latest news from uk news\n"],"metadata":{"id":"wq3FnDt5bn8O"}},{"cell_type":"code","source":["import csv\n","import requests\n","from bs4 import BeautifulSoup\n","\n","# URL of the website to scrape\n","url = 'https://www.bbc.co.uk/news'\n","\n","# Send a GET request to the URL\n","response = requests.get(url)\n","\n","# Create a BeautifulSoup object to parse the HTML content\n","soup = BeautifulSoup(response.content, 'html.parser')\n","\n","# Find all news articles on the page\n","articles = soup.find_all('div', class_='gs-c-promo')\n","\n","# Find the site name\n","site_name = soup.find('meta', property='og:site_name')['content']\n","\n","# Create a CSV file and write the header\n","with open('news_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n","    writer = csv.writer(csvfile)\n","    writer.writerow(['Title', 'Name', 'Description', 'Link'])\n","\n","    # Iterate over the articles and extract relevant information\n","    for article in articles:\n","        # Extract the title\n","        title = article.find('h3').text.strip()\n","\n","        # Extract the description (if available)\n","        description_element = article.find('p')\n","        description = description_element.text.strip() if description_element else ''\n","\n","        # Extract the link\n","        link = url + article.find('a')['href']\n","\n","        # Write the row to the CSV file\n","        writer.writerow([title, site_name, description, link])"],"metadata":{"id":"_S5sbQJqYjlm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","import requests\n","from bs4 import BeautifulSoup\n","\n","# URL of the website to scrape\n","url = 'https://newsinfo.inquirer.net/'\n","\n","# Send a GET request to the URL\n","response = requests.get(url)\n","\n","# Create a BeautifulSoup object to parse the HTML content\n","soup = BeautifulSoup(response.content, 'html.parser')\n","\n","# Find all news articles on the page\n","articles = soup.find_all('div', id='ncg-info')\n","\n","# Find the site name (if available)\n","site_name_element = soup.find('meta', property='og:title')\n","site_name = site_name_element['content'] if site_name_element else ''\n","\n","# Create a CSV file and write the header\n","with open('news_data.csv', 'w', newline='', encoding='utf-8') as csvfile:\n","    writer = csv.writer(csvfile)\n","    writer.writerow(['Title', 'Name', 'Description', 'Link'])\n","\n","    # Iterate over the articles and extract relevant information\n","    for article in articles:\n","        # Extract the title\n","        title_element = article.find('h1').find('a')\n","        title = title_element.text.strip() if title_element else ''\n","\n","        # Extract the description (if available)\n","        description = ''  # No description available in the provided HTML\n","\n","        # Extract the link\n","        link = title_element['href'] if title_element else ''\n","\n","        # Write the row to the CSV file\n","        writer.writerow([title, site_name, description, link])\n"],"metadata":{"id":"mxLAEh7DchdS"},"execution_count":null,"outputs":[]}]}